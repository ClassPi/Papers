<style>
.tag{
    color: green;
}
.caption{
    font-size: .9rem;
    letter-spacing: .1rem;
    color: gray;
}
</style>
# Vision HGNN

## 摘要

图基模型的领域已经证明了其在多种现实世界数据类型中的适应性.然而,其在一般计算机视觉任务中的应用性一直受限,直到引入视觉图神经网络（ViG）.ViG将输入图像分割为块,将这些块概念化为节点,通过连接到最近邻居来构建图.尽管如此,这种图构建方法将自身限制于简单的成对关系,导致边缘过剩和不必要的内存与计算成本.在这篇论文中,我们通过超越传统的“成对”连接并利用超图的力量来增强ViG,以封装图像信息.我们的目标是包含更多细节的块间关系.在训练和推理阶段,我们巧妙地建立并更新超图结构,使用模糊C均值方法,确保最小的计算负担.这种增强产生了视觉超图神经网络（ViHGNN）.模型的有效性通过其在图像分类和目标检测任务上的最新表现得到了实证验证,揭示了层次更高的关系学习模块.我们的代码可在以下地址获得:https://github.com/VITA-Group/ViHGNN

## 领域介绍

深度学习领域的迅速进步带来了多种计算机视觉模型的显著成功.这些包括卷积神经网络（CNNs）,视觉变换器（ViTs）,以及基于多层感知器（MLP）的视觉模型.在这些网络中,输入图像要么以欧几里得空间中的规则像素网格形式表示,要么以补丁序列的形式表示.
   
> 补丁序列通常指的是将输入图像分割成一系列较小的图像块或"补丁",然后将这些补丁 按照某种顺序排列成序列.

尽管如此,通过图结构进行更多样化的图像处理的潜力仍未被发掘.最近的发展,ViG[**<span class='tag'>18</span>**],巧妙地利用了图神经网络（GNNs）在大规模视觉任务中取得了实质性的进步.为了减少节点过多的问题,ViG借鉴了ViT的划分概念,将图像分割成更小的块,并指定每个块为一个节点.因此,ViG框架在节点及其最近邻之间建立连接,构建了一个可适应的图.进一步,图本身作为一个泛化的数据结构出现,包含网格和序列作为其更广泛图上下文中的不同实例.
虽然视觉图神经网络（ViG）的成功有效地展示了将图像作为图处理在增强灵活性和视觉感知有效性方面的优势,但使用图作为图像表示的最佳数据结构存在局限性.这些局限性根植于两个主要原因:
1. **关系的复杂性**: 一个简单图的基本限制在于它的能力仅限于连接两个节点,因此它只能适应成对的关系.这种不足在模拟高阶关系时尤为明显,而这在图像中是固有的.考虑计算机视觉中的对象识别任务,图像通常被分割成多个补丁,每个补丁代表物体的一部分.这种分割引入了复杂的补丁之间的内部和外部依赖关系,而一个简单直观的图结构难以捕捉这种复杂性.这种复杂性源于同一或不同物体的补丁之间的相互作用.因此,一个传统的图结构发现很难有效地模拟补丁之间这种多方面的关系.
2. **边的冗余生成**: 另一个与简单图表示法相关的问题是在图像描述过程中生成冗余的边.ViG通过识别每个补丁节点的最近邻居来构建其图形,并随后形成节点对之间的边.在这个过程中,所有图像补丁都被转换为特征向量,并计算特征距离以确定最近邻居.当考虑到图像中的一个对象通常是多个补丁的混合体时,这种方法就会变得有问题,因为属于同一对象的补丁产生相似的特征向量.因此,图形构建方法可能无意中产生额外的冗余边.在最坏的情况下,一个包含n个补丁的图像可能会生成n²个边,导致四次方的复杂度.

![加载失败? 请确认可以访问Github!](https://raw.githubusercontent.com/ClassPi/PicBed/master/images20240112141203.png)
<span class='caption' id='pic1'>图1.不同视觉骨干网络中建模的图像拓扑结构示意图. (a)CNN将图像视为规则的网格, (b)ViT将图像解析为全连接图, (c)ViG处理图像为带有成对边的稀疏图, (d)我们的ViHGNN将图像模型化为超图,一种"更通用"的结构.</span>

前述的考虑凸显了依赖简单图形作为图像表示的基本数据结构所固有的局限性.因此,探索能够有效解决这些问题的替代方法学是必要的,从而增强视觉感知模型的精细度.有鉴于此,我们提出了ViG的一个强大演化版本,在这个版本中,超图扮演图像表示的角色.这一创新框架,名为Vision HGNN(ViHGNN),引入了一种动态方法来改进图像表示.具体来说,超图作为一个图的广义扩展,由一系列节点和超边组成.与简单图中的成对连接不同,超图内的超边可以连接任意数量的节点.本质上,一个图可以被视为超图的一种专门形式,只考虑成对连接之间的数据点.有别于此,超图在捕捉图像内部存在的错综复杂的关联性方面表现出更优的能力,超越了成对关系的限制.要查看具体例子,请参见 <a href='#pic1'>*图1*</a>

然而，一个基本的挑战仍然存在：确定图像表示的最优超图结构。这涵盖了一个“先有鸡还是先有蛋”的问题：**一方面有利用超图进行图像表示的愿望，另一方面却缺乏一个现成可用的超图结构.** 受到ViG的图构建方法的启发，我们从利用补丁特征来构建初始超图开始。随后，初始超图生成补丁嵌入，然后这些嵌入被用于构建一个更新的超图结构。至关重要的是，补丁嵌入和超图结构经历动态更新，最终形成一个**自我强化的学习过程的"反馈循环".** 在ViHGNN框架中，我们选择**模糊C均值方法**（见第3.3节）来构建和更新超图结构，从而产生的计算开销可以忽略不计。我们的贡献可以总结如下：

1. 我们在ViG框架的基础上进一步发展，提出了一个名为ViHGNN的新范式，它将图像解释为一个动态的超图。与ViG不同，ViHGNN不仅捕获图像内的高阶关系，还减少了与图结构相关的冗余内存和计算开销。
2. 为了建立一个稳健的图像超图表示，我们在框架内无缝集成了一个自适应超图结构学习模块，从而增强了表示，同时几乎不增加开销。这种图像的超图描述为下游视觉任务提供了明显的优势。
3. 我们执行全面的实验来强调ViHGNN模型在视觉任务中的有效性，包括图像分类和对象检测。具体来说，我们的ViHGNN模型在ImageNet分类任务中实现了83.9%的顶级准确率，以及在COCO对象检测任务中实现了令人印象深刻的43.1%的平均精确度（AP）。

> 一些不太精确的数据:ResNet-50：顶级准确率（Top-1 Accuracy）大约为76.5%, Inception-v3：顶级准确率大约为 78.8%, VGG-16：顶级准确率大约为71.5%, EfficientNet-B7：顶级准确率大约为84.4%. FasterR-CNN：平均精确度（AP）大约为 37.4%,YOLOv3：平均精确度大约为33.0%, SSD300：平均精确度大约为 41.2%, EfficientDet-D7：平均精确度大约为51.5%

## 相关工作

在计算机视觉中的网络架构。卷积神经网络（CNNs）[**<span class='tag'>35</span>**, **<span class='tag'>33</span>**, **<span class='tag'>22</span>**] 曾被视为计算机视觉中的事实上的标准，用于图像分类[**<span class='tag'>35</span>**, **<span class='tag'>33</span>**], 对象检测[**<span class='tag'>47</span>**], 语义分割[**<span class='tag'>43</span>**]等多个方面。随着过去十年中快速的发展，基于残差网络的CNNs包括ResNet[**<span class='tag'>22</span>**], MobileNet[**<span class='tag'>25</span>**]和NAS搜寻的网络[**<span class='tag'>73</span>**, **<span class='tag'>84</span>**]越来越受欢迎。最近，受到NLP领域Transformer架构成功的启发，为视觉任务引入了视觉Transformer[**<span class='tag'>17</span>**, **<span class='tag'>11</span>**, **<span class='tag'>3</span>**, **<span class='tag'>4</span>**]。ViT[**<span class='tag'>11</span>**]的开创性工作直接将Transformer架构应用到非重叠的中等大小的图像块上进行图像分类。自那以后，ViT的许多变种已经被提出以提高视觉任务的性能。主要改进包括金字塔架构[**<span class='tag'>67</span>**, **<span class='tag'>41</span>**]，局部注意力[**<span class='tag'>19</span>**, **<span class='tag'>41</span>**]以及位置编码[**<span class='tag'>62</span>**]。此外，MLP架构也在计算机视觉中被探索[**<span class='tag'>54</span>**, **<span class='tag'>55</span>**, **<span class='tag'>3</span>**, **<span class='tag'>57</span>**, **<span class='tag'>15</span>**, **<span class='tag'>53</span>**]，同样，大核心CNNs也被探索[**<span class='tag'>42</span>**, **<span class='tag'>9</span>**, **<span class='tag'>40</span>**, **<span class='tag'>28</span>**]。
